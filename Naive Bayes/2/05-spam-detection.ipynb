{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection using Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = [\n",
    "    \"To use your credit, click the new WAP link in the next years txt message or click here\", \n",
    "    \"Thanks for your subscription to New Ringtone UK your new mobile will be charged £5/month Please confirm annoncement by replying\", \n",
    "    \"As a valued customer, I am pleased to advise you that following recent delivery waiting review of your Mob No. you are awarded with. Call us to review.\", \n",
    "    \"Please call our new customer service representative on\", \n",
    "    \"We are trying to contact you. Last weekends customer draw shows that you won a £1000 prize GUARANTEED. Calling years\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave one sentence from spam for testing our model later \n",
    "spam_test = [\"Customer service annoncement. You have a New Years delivery waiting for you. click\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "non = [\n",
    "    \"I don't think he goes to usf, he lives around here though\", \n",
    "    \"New car and house for my parents. i have only new job in hand\", \n",
    "    \"Great escape. I fancy the bridge but needs her lager. See you tomorrow\", \n",
    "    \"Tired. I haven't slept well the past few nights.\",\n",
    "    \"Too late. I said i have the website. I didn't i have or dont have the slippers\", \n",
    "    \"I might come by tonight then if my class lets out early\", \n",
    "    \"Jos ask if u wana meet up?\", \n",
    "    \"That would be great. We'll be at the Guild. We can try meeting with the customer on Bristol road or somewhere\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another sentence from non for testing our model \n",
    "spam_test_2 = [ \"Please call our new customer service representative on\", \n",
    "    ]\n",
    "# spam_test_2 = [\"That would be great. We'll be at the Guild. We can try meeting with the customer on Bristol road or somewhere\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = [\n",
    "#     'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and',\n",
    "#     'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being',\n",
    "#     'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\",\n",
    "#     'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    "#     'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "#     'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here',\n",
    "#     \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i',\n",
    "#     \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\",\n",
    "#     'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "#     'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought',\n",
    "#     'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she',\n",
    "#     \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than',\n",
    "#     'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "#     'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\",\n",
    "#     'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was',\n",
    "#     \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what',\n",
    "#     \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who',\n",
    "#     \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you',\n",
    "#     \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself',\n",
    "#     'yourselves'\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I might come by tonight then if my class lets out early\n",
      "I come tonight class lets early\n",
      "i come tonight class lets earli\n",
      "['i', 'come', 'tonight', 'class', 'lets', 'earli']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = non[4]\n",
    "test_sentence = non[5]\n",
    "# test_sentence = spam[1]\n",
    "\n",
    "print(test_sentence)\n",
    "\n",
    "removed_stops = remove_stopwords(test_sentence)\n",
    "print(removed_stops)\n",
    "\n",
    "\n",
    "# earlyt and earlier words , similar words make same.. stammimg\n",
    "p = PorterStemmer()\n",
    "stemmed = p.stem(removed_stops)\n",
    "print(stemmed)\n",
    "\n",
    "tokens = tokenize(stemmed)\n",
    "print(list(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  same function for \n",
    "# removing stop words\n",
    "# stemming and tokenizing\n",
    "\n",
    "def tokenize_sentence(sentence): \n",
    "    p = PorterStemmer()\n",
    "    removed_stops = remove_stopwords(sentence)\n",
    "    stemmed = p.stem(removed_stops)\n",
    "    tokens = tokenize(stemmed)\n",
    "    return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized spam:  [['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['thanks', 'subscription', 'new', 'ringtone', 'uk', 'new', 'mobile', 'charged', 'month', 'please', 'confirm', 'annoncement', 'repli'], ['as', 'valued', 'customer', 'i', 'pleased', 'advise', 'following', 'recent', 'delivery', 'waiting', 'review', 'mob', 'no', 'awarded', 'with', 'call', 'review'], ['please', 'new', 'customer', 'service', 'repres'], ['we', 'trying', 'contact', 'you', 'last', 'weekends', 'customer', 'draw', 'shows', 'won', 'prize', 'guaranteed', 'calling', 'year']]\n",
      "Tokenized non:   [['i', 'don', 't', 'think', 'goes', 'usf', 'l'], ['new', 'car', 'house', 'parents', 'new', 'job', 'hand'], ['great', 'escape', 'i', 'fancy', 'bridge', 'needs', 'lager', 'see', 'tomorrow'], ['tired', 'i', 'haven', 't', 'slept', 'past', 'nights'], ['too', 'late', 'i', 'said', 'website', 'i', 'didn', 't', 'dont', 'slipp'], ['i', 'come', 'tonight', 'class', 'lets', 'earli'], ['jos', 'ask', 'u', 'wana', 'meet', 'up'], ['that', 'great', 'we', 'll', 'guild', 'we', 'try', 'meeting', 'customer', 'bristol', 'road']]\n",
      "Dictionary:      {'lets', 'll', 'txt', 'trying', 'calling', 'awarded', 'prize', 'said', 'with', 'uk', 'fancy', 'nights', 'ask', 'try', 'tomorrow', 'won', 'years', 'month', 'recent', 'please', 'last', 'usf', 'call', 'house', 'confirm', 'needs', 'lager', 'click', 'road', 'goes', 'up', 'slept', 'don', 'use', 'tonight', 'parents', 'weekends', 'customer', 'wana', 'past', 'pleased', 'year', 'we', 'car', 'charged', 'shows', 'draw', 'annoncement', 'repres', 'meeting', 'slipp', 'late', 'new', 'see', 'come', 'repli', 'following', 'guild', 'think', 'bridge', 'great', 'mobile', 'advise', 'that', 'bristol', 'valued', 'website', 'dont', 'to', 'credit', 'thanks', 'job', 'wap', 'guaranteed', 'too', 'l', 'waiting', 'ringtone', 't', 'link', 'contact', 'meet', 'haven', 'no', 'escape', 'tired', 'as', 'review', 'u', 'jos', 'message', 'you', 'i', 'service', 'delivery', 'class', 'subscription', 'earli', 'didn', 'mob', 'hand'}\n"
     ]
    }
   ],
   "source": [
    "dictionary = set()     # will have unique values only \n",
    "spams_tokenized = [] \n",
    "nons_tokenized = [] \n",
    "\n",
    "for sentence in spam:      \n",
    "    sentence_tokens = tokenize_sentence(sentence)\n",
    "    spams_tokenized.append(sentence_tokens)\n",
    "    dictionary  = dictionary.union(sentence_tokens)   # add sentence words to the dictionary  \n",
    "\n",
    "for sentence in non:      \n",
    "    sentence_tokens = tokenize_sentence(sentence)\n",
    "    nons_tokenized.append(sentence_tokens)\n",
    "    dictionary  = dictionary.union(sentence_tokens)\n",
    "\n",
    "print(\"Tokenized spam: \", spams_tokenized)\n",
    "print(\"Tokenized non:  \", nons_tokenized)\n",
    "print(\"Dictionary:     \", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words:  101\n"
     ]
    }
   ],
   "source": [
    "# These things do not depend on an individual word so let's calculate them separately once \n",
    "\n",
    "total_word_count = len(dictionary)\n",
    "total_spam_messages = len(spams_tokenized) \n",
    "total_all_messages = len(spams_tokenized) + len(nons_tokenized)\n",
    "\n",
    "print(\"Total Number of words: \", total_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam) =  0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "# P(spam) ... does not depend on an individual word so let's calculate that separately once \n",
    "\n",
    "p_spam = total_spam_messages / total_all_messages\n",
    "\n",
    "print(\"P(spam) = \", p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count occurances \n",
    "\n",
    "def count_word_in_messages(word, messages): \n",
    "    total_count = 0\n",
    "    for msg in messages: \n",
    "        if word in msg:       # notice this ensured uniqueness automatically  \n",
    "            total_count += 1 \n",
    "            \n",
    "    return total_count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actual Probability Computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please', 'new', 'customer', 'service', 'repres']\n",
      "----------------\n",
      "Runnig for word: please\n",
      "P( w | spam)  =  0.4\n",
      "P( w )        =  0.15384615384615385\n",
      "P( spam )     =  0.38461538461538464\n",
      "P( spam | w ) =  1.0\n",
      "\n",
      "----------------\n",
      "Runnig for word: new\n",
      "P( w | spam)  =  0.6\n",
      "P( w )        =  0.3076923076923077\n",
      "P( spam )     =  0.38461538461538464\n",
      "P( spam | w ) =  0.75\n",
      "\n",
      "----------------\n",
      "Runnig for word: customer\n",
      "P( w | spam)  =  0.6\n",
      "P( w )        =  0.3076923076923077\n",
      "P( spam )     =  0.38461538461538464\n",
      "P( spam | w ) =  0.75\n",
      "\n",
      "----------------\n",
      "Runnig for word: service\n",
      "P( w | spam)  =  0.2\n",
      "P( w )        =  0.07692307692307693\n",
      "P( spam )     =  0.38461538461538464\n",
      "P( spam | w ) =  1.0\n",
      "\n",
      "----------------\n",
      "Runnig for word: repres\n",
      "P( w | spam)  =  0.2\n",
      "P( w )        =  0.07692307692307693\n",
      "P( spam )     =  0.38461538461538464\n",
      "P( spam | w ) =  1.0\n",
      "\n",
      "P( spam | all_words ) =  0.5625\n"
     ]
    }
   ],
   "source": [
    "final_prob = 1   # can't start from 0 \n",
    "\n",
    "\n",
    "for test_sentence in spam_test_2: \n",
    "# for test_sentence in spam_test_2: \n",
    "    test_sentence = tokenize_sentence(test_sentence)\n",
    "    print(test_sentence)\n",
    "    \n",
    "    # let's run this for each word separately \n",
    "    for word in test_sentence: \n",
    "        print(\"----------------\")\n",
    "        print(\"Runnig for word:\", word)\n",
    "        \n",
    "        # Find P( w | spam)\n",
    "        spam_count = count_word_in_messages(word, spams_tokenized)\n",
    "        p_w_spam = spam_count / total_spam_messages \n",
    "        print(\"P( w | spam)  = \", p_w_spam)\n",
    "        \n",
    "        # Find P( w )\n",
    "        w_count = count_word_in_messages(word, spams_tokenized)\n",
    "        w_count += count_word_in_messages(word, nons_tokenized)\n",
    "        p_w = w_count / total_all_messages\n",
    "        print(\"P( w )        = \", p_w)\n",
    "        \n",
    "        \n",
    "        # Find P( spam | w )\n",
    "        p_spam_w = (p_w_spam * p_spam) / p_w\n",
    "        print(\"P( spam )     = \", p_spam)\n",
    "        print(\"P( spam | w ) = \", p_spam_w)\n",
    "        print(\"\")\n",
    "        final_prob *= p_spam_w\n",
    "        \n",
    "        \n",
    "    print(\"P( spam | all_words ) = \", final_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
